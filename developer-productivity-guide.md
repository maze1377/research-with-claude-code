# Developer Productivity with AI Coding Assistants

**Best practices for Cursor, Claude Code, Windsurf, and autonomous agents**

**Last Updated:** 2025-12-26 | Version 1.0

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Cursor IDE Best Practices](#1-cursor-ide-best-practices)
3. [Claude Code CLI Best Practices](#2-claude-code-cli-best-practices)
4. [Windsurf & Cascade](#3-windsurf--cascade)
5. [Autonomous Agents (Devin)](#4-autonomous-agents-devin)
6. [Production Workflows](#5-production-workflows)
7. [Team Collaboration](#6-team-collaboration)
8. [Performance Optimization](#7-performance-optimization)
9. [Security Considerations](#8-security-considerations)
10. [Common Pitfalls](#9-common-pitfalls)
11. [Quick Reference](#10-quick-reference)

---

## Executive Summary

### Market Context (December 2025)

| Metric | Value | Source |
|--------|-------|--------|
| Developers using AI coding assistants weekly | **82%** | [Stack Overflow 2025](https://visualstudiomagazine.com/articles/2025/08/01/stack-overflow-dev-survey-visual-studio-vs-code-hold-of-ai-ides-to-remain-on-top.aspx) |
| All code globally that is AI-generated | **41%** | GitHub |
| Google's new code generated by AI | **25%** | Google |
| AI coding tools market size | **$4.8B** (projected $17.2B by 2030) | [CB Insights](https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/) |
| Individual productivity increase | 20-40% | Industry Research |
| AI code containing security vulnerabilities | **45%** | Veracode 2025 |
| AI-generated code with design flaws | 62% | Cloud Security Alliance |
| Java AI code with security failures | **70%** | Veracode |
| AI PRs with more defects than human | 70% more | CodeRabbit |
| Developer positive sentiment (down from 70%+) | **60%** | Stack Overflow |

### Market Leaders (December 2025)

| Tool | Market Position | Notes |
|------|-----------------|-------|
| **ChatGPT** | 82% usage | Primary entry point for developers |
| **GitHub Copilot** | 68% usage | Leading code completion |
| **Cursor** | 18% usage (AI IDEs) | $9.9B valuation; 45% of Y Combinator companies |
| **Claude Code** | 10% usage (AI IDEs) | $1B+ ARR threshold crossed |
| **Windsurf** | 5% usage (AI IDEs) | Growing flow-aware IDE |

> **Market Shift (August 2025):** Cursor overtook GitHub Copilot in organizational adoption rates (43% vs 37%).

### Key Insight

> "AI tools demonstrably increase individual developer output by 20-40%, but realizing company-level delivery gains requires systematic process changes, rigorous governance, and strategic investments in team capabilities."

### Tool Comparison

| Tool | Type | Best For | Learning Curve |
|------|------|----------|----------------|
| **Cursor** | Full IDE | Complex projects, multi-file editing | Medium |
| **Claude Code** | CLI | Terminal-first workflows, MCP integration | Medium |
| **Windsurf/Cascade** | Full IDE | Flow-aware development, agentic tasks | Low-Medium |
| **GitHub Copilot** | Extension | Quick completions, existing IDE users | Low |
| **Devin** | Autonomous | Well-scoped tasks, parallel execution | Medium |

---

## 1. Cursor IDE Best Practices

### Configuration Architecture

Cursor uses a three-tier configuration hierarchy:

```
┌─────────────────────────────────────────────────────────────────┐
│                    CONFIGURATION HIERARCHY                       │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  GLOBAL RULES (~/.cursor/rules/)                           ││
│  │  Personal preferences across all projects                   ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  PROJECT RULES (.cursor/rules/)                            ││
│  │  Team-shared patterns, committed to repo                    ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  LOCAL RULES (.cursor/rules/.local/)                       ││
│  │  Personal experiments, not committed                        ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘
```

### .mdc File Format (Replaces .cursorrules)

The new `.mdc` format in `.cursor/rules/` directory:

```yaml
---
name: "TypeScript Patterns"
version: "1.0"
globs: ["src/**/*.ts", "src/**/*.tsx"]
alwaysInclude: false
autoAttach: true
---

# TypeScript Development Patterns

## Naming Conventions
- Use PascalCase for components and types
- Use camelCase for functions and variables
- Use UPPER_SNAKE_CASE for constants

## Error Handling
Always use custom error types:

```typescript
class DomainError extends Error {
  constructor(
    message: string,
    public readonly code: string,
    public readonly cause?: Error
  ) {
    super(message);
    this.name = 'DomainError';
  }
}
```

## Why These Patterns
We've had production incidents from type confusion. Strict TypeScript
prevents similar issues. Always enable strict mode.
```

### Rule File Organization

**Recommended Structure:**

```
.cursor/
└── rules/
    ├── code-style.mdc       # Formatting, naming conventions
    ├── architecture.mdc      # Project structure, patterns
    ├── testing.mdc          # Test conventions, frameworks
    ├── security.mdc         # Security requirements
    ├── api-standards.mdc    # API design patterns
    └── .local/
        └── experiments.mdc  # Personal experiments
```

### Cursor Modes

| Mode | Access | Best For | Behavior |
|------|--------|----------|----------|
| **Chat** | Cmd+L | Understanding, explanations | Read-only, explains |
| **Composer** | Cmd+I | Multi-file implementation | Direct modification |
| **Agent** | Cmd+I → Agent | Complex workflows, terminal | Autonomous execution |

**Mode Selection Guide:**

```
What do you need?
│
├── Understanding existing code → Chat (Cmd+L)
├── Implementing known feature → Composer (Cmd+I)
├── Complex task with terminal → Agent Mode
└── Quick inline edit → Inline Edit (Cmd+K)
```

### Essential Keyboard Shortcuts

| Action | Mac | Windows | Use Case |
|--------|-----|---------|----------|
| Open Chat | Cmd+L | Ctrl+L | Ask questions |
| Open Composer | Cmd+I | Ctrl+I | Multi-file changes |
| Inline Edit | Cmd+K | Ctrl+K | Modify selected code |
| Toggle Terminal | Cmd+` | Ctrl+` | Run commands |
| New Chat Tab | Cmd+T | Ctrl+T | Separate conversation |
| Command Palette | Cmd+Shift+P | Ctrl+Shift+P | All commands |
| File Reference | @filename | @filename | Add specific file to context |
| Folder Reference | @folder | @folder | Add directory to context |
| Codebase Query | @codebase | @codebase | Search entire project |

### Context Management

**Problem:** Context window (~200K tokens) fills quickly with large codebases.

**Solution:** Explicit context inclusion:

```
Best Practice:
@src/services/auth.ts           # Specific file
@src/components/Login.tsx       # Related component
@instructions.md                # Project context

Avoid:
@codebase                       # Unless needed
Including entire directories     # Too much noise
```

**Create Context Files:**

```markdown
<!-- instructions.md -->
# Project Overview
E-commerce platform with React frontend, Node backend.

# Architecture
- Frontend: React + TypeScript + TanStack Query
- Backend: Express + PostgreSQL + Redis
- Auth: JWT with refresh tokens

# Key Patterns
- All API calls through /src/api/client.ts
- State management via React Query (no Redux)
- Error boundaries around all route components
```

### Agent Mode Best Practices

**When to Use Agent Mode:**

| Scenario | Use Agent Mode? |
|----------|-----------------|
| Simple code completion | No → Use inline edit |
| Multi-step implementation | Yes |
| Requires running tests | Yes |
| Needs terminal commands | Yes |
| Understanding code only | No → Use Chat |

**Agent Mode Workflow:**

```
1. Clear, specific request
   "Implement user authentication with JWT, include tests"

2. Let agent work
   - Creates files
   - Runs commands
   - Checks results

3. Review output
   - Accept/reject changes
   - Request adjustments

4. Verify
   - Run tests yourself
   - Check edge cases
```

### Cursor 2.0: Agent-Centric Development

**Released:** October 29, 2025 — Major update pairing purpose-built Composer model with agent-centered interface.

**Key Features:**

| Feature | Description |
|---------|-------------|
| **Parallel Agents** | Run up to **8 coding agents simultaneously** in isolated environments |
| **Background Agents** | Work on separate branches, can open PRs; "AI pair programmers in isolated Ubuntu VMs" |
| **Composer Model** | 4x faster than similarly intelligent models; completes most tasks in <30s |
| **Browser Tool** | Agents can test web applications, capture screenshots, identify issues |
| **Voice Mode** | Speech-to-text for controlling agents with custom trigger keywords |

```
┌─────────────────────────────────────────────────────────────────┐
│                 PARALLEL AGENTS ARCHITECTURE                     │
│                                                                  │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │    Agent 1      │  │    Agent 2      │  │    Agent 3      │ │
│  │                 │  │                 │  │                 │ │
│  │  worktree-1/    │  │  worktree-2/    │  │  worktree-3/    │ │
│  │  Feature A      │  │  Feature B      │  │  Bug Fix C      │ │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘ │
│           │                    │                    │           │
│           └────────────────────┼────────────────────┘           │
│                                ▼                                │
│                        Main Repository                          │
│                       (shared .git dir)                         │
└─────────────────────────────────────────────────────────────────┘
```

**Multi-Model Comparison:**
Run the same task through multiple models (GPT-5, Claude Sonnet 4.5, Composer) to compare approaches—GPT-5 prioritizes error handling, Claude focuses on clean structure, Composer aims for minimal changes.

**Background Agents:**
- Spin up from Cursor, Slack, or web/mobile
- Work on separate branches
- Can open PRs for review
- 99.9% reliability for cloud agents

**Setup:**
```bash
# Create worktree for parallel work
git worktree add ../feature-auth -b feature/auth

# Agent works in isolated copy
cd ../feature-auth
# (Start Cursor agent here)
```

**Best Practices:**
- Use for independent, parallelizable tasks
- Merge completed worktrees promptly
- Clean up unused worktrees regularly

---

## 2. Claude Code CLI Best Practices

### CLAUDE.md Architecture

> **"CLAUDE.md is the single most impactful optimization for Claude Code"** — Anthropic Best Practices

Claude Code uses markdown files for persistent memory:

```
┌─────────────────────────────────────────────────────────────────┐
│                    CLAUDE.md HIERARCHY                           │
│                                                                  │
│  ~/.claude/CLAUDE.md                                            │
│  └── Personal preferences across all projects                   │
│                                                                  │
│  ~/project/CLAUDE.md                                            │
│  └── Team-shared, version controlled                            │
│                                                                  │
│  ~/project/frontend/CLAUDE.md                                   │
│  └── Subsystem-specific context                                 │
│                                                                  │
│  ~/project/backend/CLAUDE.md                                    │
│  └── Subsystem-specific context                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Key Difference from Cursor:** CLAUDE.md files are:
- Version controlled (shared with team)
- Loaded automatically at session start
- Updated via `#` command during conversation

### CLAUDE.md Template

```markdown
# Project: E-Commerce Platform

## Overview
Modern e-commerce platform serving 50K daily users.
React frontend, Node.js backend, PostgreSQL database.

## Architecture
- Frontend: `src/web/` - React 18, TypeScript, Vite
- Backend: `src/api/` - Express, Prisma ORM
- Shared: `src/shared/` - Types, utilities

## Coding Standards

### TypeScript
- Strict mode enabled
- Prefer `interface` over `type` for objects
- Always define return types for functions

### Error Handling
```typescript
// Always use Result pattern for fallible operations
type Result<T, E = Error> = { ok: true; value: T } | { ok: false; error: E };
```

### Testing
- Jest for unit tests, Playwright for E2E
- Minimum 80% coverage for new code
- Co-locate tests with source: `foo.ts` → `foo.test.ts`

## Common Commands
```bash
npm run dev          # Start development
npm run test         # Run tests
npm run lint         # Lint check
npm run build        # Production build
```

## Key Files
- `src/api/routes/` - API endpoints
- `src/web/pages/` - Page components
- `prisma/schema.prisma` - Database schema

## Recent Decisions
- 2025-12-20: Switched from Redux to TanStack Query
- 2025-12-15: Added rate limiting to all endpoints
```

### Updating CLAUDE.md During Conversation

Use `#` command to add learnings:

```
User: # Always wrap API responses in try-catch blocks

Claude: I'll add that to your CLAUDE.md file.
```

This creates persistent memory that survives sessions.

### Custom Slash Commands

Create reusable commands in `.claude/commands/`:

```markdown
<!-- .claude/commands/review.md -->
# Code Review Command

Perform a comprehensive code review:

1. Check for security vulnerabilities
2. Verify error handling
3. Ensure test coverage
4. Review naming conventions
5. Check for performance issues

Focus on files changed in the current branch:
```bash
git diff --name-only main
```

Provide specific, actionable feedback.
```

**Usage:** `/review` in conversation

### Claude Code Hooks

Configure automated actions in settings:

```json
{
  "hooks": {
    "post-file-write": [
      "prettier --write {file}",
      "eslint --fix {file}"
    ],
    "pre-commit": [
      "npm run lint",
      "npm run test"
    ]
  }
}
```

### MCP Server Integration

Model Context Protocol enables tool integration:

```json
{
  "mcpServers": {
    "database": {
      "command": "mcp-server-postgres",
      "args": ["--connection-string", "$DATABASE_URL"]
    },
    "github": {
      "command": "mcp-server-github",
      "env": {
        "GITHUB_TOKEN": "$GITHUB_TOKEN"
      }
    }
  }
}
```

**Configuration Locations:**
| Location | Scope | Use Case |
|----------|-------|----------|
| `.mcp.json` (project root) | Project | Team-shared, version controlled |
| `.claude/settings.local.json` | Project | Personal project-specific |
| `~/.claude/settings.local.json` | User | Personal tooling across projects |

**Popular MCP Servers (2025):**
- `mcp-server-filesystem` - File operations
- `mcp-server-github` - GitHub integration
- `mcp-server-postgres` - Database queries
- `mcp-server-slack` - Slack messaging
- `mcp-server-browserbase` - Web scraping
- `Sequential Thinking MCP` - Enhanced problem-solving
- `Puppeteer MCP` - Web automation

**⚠️ Security Warning:** Use third-party MCP servers at your own risk. Be especially careful with servers that fetch untrusted content (prompt injection risk).

**Debugging MCP:**
```bash
# Launch with debug flag
claude --mcp-debug

# Check server status
/mcp  # Shows "connected" or "failed" for each server
```

### Session Memory Management

Claude Code automatically saves session memory:

```
Location: ~/.claude/session-memory/[session-id].md
Triggers:
- After ~10,000 tokens
- Every ~5,000 tokens thereafter
- After every 3 tool calls
```

**Review session memory:**
```bash
ls ~/.claude/session-memory/
cat ~/.claude/session-memory/latest.md
```

---

## 3. Windsurf & Cascade

### Cascade's Flow Awareness

Cascade tracks all developer actions:
- File edits
- Terminal commands
- Clipboard activity
- Conversation history

**Benefit:** Reduced context re-explanation

```
Traditional AI:
"I modified auth.ts to add JWT validation, then ran tests
which failed because of missing mock. I added the mock..."

Cascade:
"Continue my work"
(Cascade already knows what you did)
```

### Cascade Modes

| Mode | Purpose | File Modification |
|------|---------|-------------------|
| **Write** | Create/modify code | Yes |
| **Chat** | Questions, explanations | No |

**Switch modes:** Cmd+L or click interface icon

### .codeiumignore Configuration

Protect sensitive or irrelevant files:

```gitignore
# .codeiumignore

# Sensitive files
.env*
*.pem
credentials/

# Large irrelevant directories
node_modules/
dist/
.next/

# Legacy code (don't touch)
legacy/
deprecated/
```

**Global ignore:** `~/.codeium/.codeiumignore`

### Cascade Workflow Tips

**Tool Call Limits:**
- Up to 25 tool calls per prompt
- Type `continue` to resume after limit
- Queue messages while working

**Best Practices:**

```
1. Start with Chat mode to understand
2. Switch to Write mode to implement
3. Let Cascade detect your environment
4. Use `continue` for long workflows
```

### Windsurf vs Cursor

| Feature | Windsurf | Cursor |
|---------|----------|--------|
| Flow awareness | Built-in tracking | Manual context |
| Configuration | .codeiumignore | .mdc rules |
| Modes | Write/Chat | Chat/Composer/Agent |
| Tool calls | 25 per prompt | Varies |
| Auto-detection | Strong (deps, build) | Good |
| Parallel agents | Coming (2.0) | Yes (git worktrees) |

### Cascade 2.0 (Coming Soon)

Windsurf is developing **Cascade 2.0** with:
- Multi-agent collaboration capabilities
- Enhanced flow awareness across agents
- Improved parallel task execution

**Current Status:** Beta expected Q1 2025

---

## 4. Autonomous Agents (Devin)

### Appropriate Use Cases

**Devin excels at:**
- Well-scoped tasks (4-8 hour junior engineer work)
- Clear acceptance criteria
- Verifiable outcomes

| Good Fit | Poor Fit |
|----------|----------|
| Code reviews (first pass) | Ambiguous requirements |
| Unit test writing | Architectural decisions |
| Codebase migrations | Complex debugging |
| Security vulnerability fixes | Creative problem-solving |
| Documentation generation | Mid-task requirement changes |

### Performance Benchmarks (December 2025)

| Metric | Current | Previous Year | Improvement |
|--------|---------|---------------|-------------|
| Problem-solving speed | 4x faster | Baseline | 4x |
| Resource efficiency | 2x better | Baseline | 2x |
| PR merge rate | **67%** | 34% | ~2x |
| Security fix time | 1.5 min | 30 min (human) | **20x efficiency** |
| SWE-bench (unassisted) | **13.86%** | 1.96% (prior best) | 7x |
| Total PRs merged | Hundreds of thousands | N/A | Production scale |

**Enterprise Adoption:**
- Goldman Sachs, Santander, Nubank among users
- [Bilt](https://cognition.ai/blog/devin-annual-performance-review-2025): 800+ PRs merged, >50% acceptance rate
- [Ramp](https://cognition.ai/blog/devin-annual-performance-review-2025): Up to 80 PRs/week for tech debt cleanup
- [Nubank](https://cognition.ai/blog/devin-annual-performance-review-2025): 12x efficiency improvement, 20x cost savings

**Key Achievement:** Devin's PR merge rate doubled from 34% to 67% in 2025, making it viable for production workflows with appropriate oversight.

**Reality Check:** Independent testing (Answer.AI) found 15% success rate (3/20 tasks). Devin is "senior-level at codebase understanding but junior at execution."

### Fleet Deployment Pattern

Run multiple Devin instances in parallel:

```
┌─────────────────────────────────────────────────────────────────┐
│                     FLEET ARCHITECTURE                           │
│                                                                  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │   Devin 1   │  │   Devin 2   │  │   Devin 3   │              │
│  │             │  │             │  │             │              │
│  │ Migrate     │  │ Migrate     │  │ Migrate     │              │
│  │ Module A    │  │ Module B    │  │ Module C    │              │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘              │
│         │                │                │                      │
│         └────────────────┼────────────────┘                      │
│                          ▼                                       │
│                   Code Review                                    │
│                   (Human or AI)                                  │
└─────────────────────────────────────────────────────────────────┘
```

**Use Case:** Angular → React migration across 50 repositories

### Oversight Strategies

**Sampling-Based Review:**
- Don't review every change
- Sample 10-20% for detailed review
- Full review for security-sensitive changes

**Automated Guardrails:**
- Required test coverage thresholds
- Static analysis gates
- Security scanning

### When NOT to Use Autonomous Agents

| Scenario | Why |
|----------|-----|
| Requirements unclear | Can't iterate mid-task effectively |
| Novel architecture | Needs human judgment |
| Performance-critical | May not optimize well |
| Security-critical | Requires expert review |

---

## 5. Production Workflows

### Testing Strategy for AI-Generated Code

**Traditional TDD (Red-Green-Refactor) is inefficient with AI.**

**Recommended: Comprehensive Test-First Approach:**

```
1. Generate ALL tests for the feature first
   "Generate comprehensive tests for user authentication:
    - Happy path: valid login
    - Edge cases: expired tokens, invalid credentials
    - Error handling: network failures, rate limiting"

2. Implement to pass all tests
   "Implement authentication to pass all tests above"

3. Review and iterate
   - Fix any failing tests
   - Add missed edge cases
```

### Acceptance Test-Driven Development (ATDD)

```markdown
## Feature: User Login

### Scenario: Successful login with valid credentials
Given a registered user with email "user@example.com"
When they submit valid password
Then they receive a JWT token
And the token expires in 24 hours

### Scenario: Failed login with invalid password
Given a registered user
When they submit wrong password
Then they receive 401 error
And no token is issued
And rate limit counter increases

### Scenario: Account lockout after failures
Given a user with 4 failed attempts
When they fail a 5th time
Then account is locked for 15 minutes
And notification email is sent
```

Generate tests from these scenarios, then implement.

### Code Review for AI Code

**Multi-Stage Validation:**

```
┌─────────────────────────────────────────────────────────────────┐
│                    CODE REVIEW PIPELINE                          │
│                                                                  │
│  Stage 1: AUTOMATED CHECKS                                       │
│  ├── Linting (ESLint, Prettier)                                 │
│  ├── Type checking (TypeScript)                                 │
│  ├── Static analysis (SonarQube)                                │
│  └── Security scanning (Snyk, OWASP)                            │
│                                                                  │
│  Stage 2: AI CODE REVIEW                                        │
│  ├── Logic error detection                                       │
│  ├── Pattern compliance                                          │
│  └── Architectural fit                                           │
│                                                                  │
│  Stage 3: HUMAN REVIEW (if passed above)                        │
│  ├── Business logic correctness                                  │
│  ├── Architectural decisions                                     │
│  └── Security-sensitive areas                                    │
└─────────────────────────────────────────────────────────────────┘
```

**Key Statistic:** 62% of AI-generated code has design flaws or security vulnerabilities.

### CI/CD Integration

```yaml
# .github/workflows/ai-code-check.yml
name: AI Code Quality Check

on: [pull_request]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Stage 1: Automated checks
      - name: Lint
        run: npm run lint

      - name: Type Check
        run: npm run typecheck

      - name: Security Scan
        run: npm audit --audit-level=high

      # Stage 2: AI review (optional)
      - name: AI Code Review
        uses: coderabbit-ai/review@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}

      # Stage 3: Quality gates
      - name: Test Coverage
        run: npm run test -- --coverage

      - name: Coverage Threshold
        run: |
          coverage=$(jq '.total.lines.pct' coverage/coverage-summary.json)
          if (( $(echo "$coverage < 80" | bc -l) )); then
            echo "Coverage $coverage% is below 80% threshold"
            exit 1
          fi
```

---

## 6. Team Collaboration

### AI Tool Governance Policy Template

```markdown
# AI Coding Tools Policy

## Approved Tools
- Cursor (Enterprise license)
- Claude Code (Team license)
- GitHub Copilot (Enterprise)

## Prohibited Tools
- Consumer ChatGPT (code exposure risk)
- Unvetted browser extensions
- Tools without SOC2 compliance

## Usage Guidelines

### Must Do
1. Review all AI-generated code before committing
2. Run security scans on AI changes
3. Disclose AI usage in PR descriptions
4. Use approved context/rules files

### Must Not
1. Paste proprietary code into consumer AI tools
2. Accept AI suggestions without review
3. Disable security scans for AI code
4. Share API keys or credentials with AI

## Code Attribution
Include in commit message when AI-assisted:
```
feat: Add user authentication

AI-assisted: Cursor Agent Mode
Human review: [Your name]
```

## Incident Reporting
If AI tool produces concerning output:
1. Document the prompt and response
2. Report to security team
3. Do not commit the code
```

### Shared Prompt Library

Create team-wide prompt patterns:

```markdown
<!-- .team/prompts/new-api-endpoint.md -->
# New API Endpoint Prompt

Create a new REST API endpoint with the following requirements:

## Endpoint Details
- Method: [GET/POST/PUT/DELETE]
- Path: [/api/v1/...]
- Description: [What this endpoint does]

## Requirements
1. Use our standard Express middleware chain
2. Validate input with Zod schemas
3. Return standard response format:
   ```json
   { "success": true, "data": {...} }
   ```
4. Include error handling with our error codes
5. Add OpenAPI documentation comments
6. Write unit tests achieving 80% coverage

## Reference Files
@src/api/middleware/auth.ts
@src/api/utils/response.ts
@src/api/schemas/index.ts
```

### Team Training Program

| Level | Content | Duration | Audience |
|-------|---------|----------|----------|
| **L1: Basics** | What AI tools do, basic usage | 2 hours | All devs |
| **L2: Intermediate** | Context management, modes, shortcuts | 4 hours | Regular users |
| **L3: Advanced** | Prompt engineering, meta-prompting | 8 hours | Power users |
| **L4: Admin** | Security, governance, cost management | 4 hours | Team leads |

### Knowledge Sharing Practices

**Weekly Sync (15 min):**
- What prompts worked well?
- What AI mistakes did we catch?
- New patterns to share?

**Slack Channel:** `#ai-tools-tips`
- Share successful prompts
- Report issues
- Discuss new techniques

**Documentation:**
- Update shared rules files monthly
- Document AI limitations specific to your codebase
- Track cost per developer/project

---

## 7. Performance Optimization

### Context Caching by Provider

| Provider | Cache Duration | Minimum Size | Cost Reduction |
|----------|---------------|--------------|----------------|
| OpenAI | 1 hour | Auto | 50-90% |
| Anthropic | 5-60 min | Varies | 50-90% |
| Google | 24 hours | 32K tokens | 50-90% |

**Caching Strategy:**

```
┌─────────────────────────────────────────────────────────────────┐
│                     OPTIMAL PROMPT STRUCTURE                     │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  STATIC (Cached)                                           ││
│  │  System instructions, examples, context                     ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  DYNAMIC (Not Cached)                                       ││
│  │  User query, variable content                               ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘

WRONG:
[User query] + [System instructions] + [Examples]

RIGHT:
[System instructions] + [Examples] + [User query]
```

### Context Management Strategies

| Strategy | Cost Reduction | Success Rate Impact |
|----------|---------------|---------------------|
| Observation masking | 50%+ | Neutral |
| LLM summarization | 40-50% | Slight decrease |
| Hybrid (mask + summarize) | 55-60% | +2.6% |

**Observation Masking:**
```
Replace: "File auth.ts was read. Contents: [2000 tokens of code]..."
With: "File auth.ts was read at step 47"
```

### Token Optimization Techniques

| Technique | Savings | Effort |
|-----------|---------|--------|
| Concise prompting | 20-30% | Low |
| Remove redundant context | 30-40% | Low |
| Use model-appropriate tier | 40-60% | Low |
| Implement RAG | 70% | Medium |
| Batch non-urgent requests | 50% discount | Medium |

### Cost Monitoring

Track per-developer and per-project:

```javascript
// Example cost tracking
const costTracker = {
  logUsage(developer, project, tokens, model) {
    const cost = this.calculateCost(tokens, model);
    this.db.insert({
      timestamp: new Date(),
      developer,
      project,
      inputTokens: tokens.input,
      outputTokens: tokens.output,
      model,
      cost
    });
  },

  calculateCost(tokens, model) {
    const rates = {
      'gpt-4o': { input: 0.0025, output: 0.01 },
      'gpt-4o-mini': { input: 0.00015, output: 0.0006 },
      'claude-sonnet': { input: 0.003, output: 0.015 }
    };
    const rate = rates[model];
    return (tokens.input * rate.input + tokens.output * rate.output) / 1000;
  }
};
```

---

## 8. Security Considerations

### AI Code Security Statistics (2025)

| Metric | Value | Source |
|--------|-------|--------|
| AI-generated code with security flaws | **45%** | Veracode |
| Java AI code with vulnerabilities | **70%** | Veracode |
| Python AI code with vulnerabilities | 30-40% | Veracode |
| Security scan failures for AI code | 4x higher than human | Industry Research |

> **Critical Finding:** Nearly half of all AI-generated code contains security vulnerabilities. Java has the highest failure rate at 70%, requiring extra scrutiny for enterprise Java projects.

### Prompt Injection Risks

**The Scale of the Problem:**
- **35%** of all real-world AI security incidents in 2025 caused by simple prompts
- Some incidents led to **$100K+** losses without writing a single line of code
- OpenAI admits prompt injection "is unlikely to ever be fully 'solved'"

**Real-World Incidents (2025):**
| Incident | Impact |
|----------|--------|
| Fortune 500 Financial Services | Customer service AI leaked sensitive data for weeks; millions in fines |
| [Salesforce Agentforce](https://noma.security/blog/forcedleak-agent-risks-exposed-in-salesforce-agentforce) | CVSS 9.4 vulnerability; CRM data exfiltration via prompt injection |
| Docker Hub AI Assistant | Data exfiltration via poisoned repository metadata |
| Amazon Q VS Code | Wiped local files, disrupted AWS infrastructure |
| [AI Browser Vulnerabilities](https://brave.com/blog/unseeable-prompt-injections/) | Invisible prompt injections in screenshots (systemic issue) |

### Attack Vectors

| Vector | Description | Mitigation |
|--------|-------------|------------|
| **File-based injection** | Malicious instructions in processed files | Input validation, sandboxing |
| **MCP exploitation** | Compromised MCP server | Server verification, access control |
| **Context poisoning** | Injecting instructions via clipboard | Context isolation |
| **Agent hijacking** | Redirecting autonomous agent behavior | Behavioral monitoring |

### Defense Layers

```
┌─────────────────────────────────────────────────────────────────┐
│                     SECURITY LAYERS                              │
│                                                                  │
│  1. INPUT VALIDATION                                             │
│     └── Scan processed documents for embedded instructions       │
│                                                                  │
│  2. AUTHORIZATION                                                │
│     └── Least-privilege access for AI agents                    │
│     └── Role-based access control                                │
│                                                                  │
│  3. BEHAVIORAL MONITORING                                        │
│     └── Baseline normal agent behavior                          │
│     └── Alert on anomalies                                       │
│                                                                  │
│  4. OUTPUT FILTERING                                             │
│     └── Block sensitive data exfiltration                       │
│     └── Prevent destructive commands                            │
│                                                                  │
│  5. AUDIT LOGGING                                                │
│     └── Log all AI tool actions                                 │
│     └── Enable forensic analysis                                │
└─────────────────────────────────────────────────────────────────┘
```

### Security Checklist

| Check | Implementation |
|-------|----------------|
| No secrets in prompts | Environment variables, secret managers |
| Limited file access | .codeiumignore, permission restrictions |
| No credential sharing | Never paste API keys to AI |
| MCP server verification | Review server source, limit permissions |
| Action logging | Track all AI-initiated actions |
| Anomaly detection | Monitor for unusual patterns |

### Secure Configuration

```json
// Example security configuration
{
  "security": {
    "allowedDirectories": [
      "src/",
      "tests/"
    ],
    "blockedPatterns": [
      "*.pem",
      "*.key",
      ".env*",
      "credentials/*"
    ],
    "commandWhitelist": [
      "npm test",
      "npm run lint",
      "git status"
    ],
    "requireApproval": [
      "npm publish",
      "git push",
      "rm -rf"
    ]
  }
}
```

---

## 9. Common Pitfalls

### Pitfall 1: Over-Reliance Without Verification

**The Problem:**
- **45%** of AI-generated code has security vulnerabilities (Veracode 2025)
- 62% of AI code has design flaws or vulnerabilities
- 70% more defects in AI PRs vs human PRs
- 76% of developers have low confidence in AI output

**The Solution:**
- Always review before commit
- Run comprehensive tests
- Use multi-stage validation pipeline

### Pitfall 2: Code Quality Degradation

**Pattern:** Individual velocity up, system quality down

**Why:** AI optimizes for "code that works" not "code that's maintainable"

**Signs:**
- Increasing tech debt
- More production incidents
- Harder to understand codebase

**Solution:**
- Invest in quality gates proportional to AI usage
- Static analysis on every PR
- Regular architectural review

### Pitfall 3: Architectural Drift

**Research Finding:** AI defaults to:
- Monolithic designs (vs microservices)
- Reimplementing vs using libraries
- "Vanilla style" without abstraction

**Solution:**
- Explicit architectural guidance in context
- Require library usage in prompts
- Regular architecture reviews

### Pitfall 4: Insufficient Context

**Signs:**
- AI generates code inconsistent with project style
- Frequent "that's not how we do it here"
- Low reuse of generated code

**Solution:**
- Invest in CLAUDE.md / .mdc files
- Create comprehensive instructions.md
- Regularly update context with learnings

### Pitfall 5: Shadow AI Usage

**Risk:** Developers using unapproved consumer AI tools

**Consequences:**
- Code exposure to third parties
- No enterprise security controls
- Compliance violations

**Solution:**
- Clear approved tool list
- Block consumer AI at network level
- Provide good alternatives

### Anti-Pattern Checklist

| Anti-Pattern | Detection | Fix |
|--------------|-----------|-----|
| Accept without review | No review comments on AI PRs | Require reviews |
| No tests for AI code | Low coverage on new files | Coverage gates |
| Ignoring AI mistakes | Same bugs recurring | Document limitations |
| Monolithic generation | Large, complex PRs | Break into smaller tasks |
| Copy-paste prompting | No context files | Invest in configuration |

---

## 10. Quick Reference

### Tool Selection

| Need | Tool |
|------|------|
| Full IDE replacement | Cursor or Windsurf |
| Terminal-first workflow | Claude Code |
| Existing VS Code user | GitHub Copilot |
| Autonomous task execution | Devin |
| Quick prototyping | Any (Cursor fastest) |

### Keyboard Shortcuts Cheat Sheet

**Cursor:**
| Action | Shortcut |
|--------|----------|
| Chat | Cmd+L |
| Composer | Cmd+I |
| Inline Edit | Cmd+K |
| Terminal | Cmd+` |

**Claude Code:**
| Action | Shortcut |
|--------|----------|
| Add to memory | # [text] |
| Custom command | /[command] |
| Clear context | /clear |

### Configuration Quick Start

**Cursor:**
```bash
mkdir -p .cursor/rules
touch .cursor/rules/project.mdc
```

**Claude Code:**
```bash
touch CLAUDE.md
mkdir -p .claude/commands
```

**Windsurf:**
```bash
touch .codeiumignore
```

### Testing Strategy

```
1. Generate comprehensive tests FIRST
2. Implement to pass tests
3. Review generated tests for correctness
4. Run with coverage
5. Add missed edge cases
```

### Security Essentials

| Rule | Implementation |
|------|----------------|
| Never share secrets | Use env vars |
| Limit file access | Use ignore files |
| Review all output | Human check required |
| Log everything | Audit trail |
| Use approved tools | No shadow AI |

### Cost Optimization

| Technique | Savings |
|-----------|---------|
| Static content first | 50-90% (caching) |
| Concise prompts | 20-30% |
| Smaller models when possible | 40-60% |
| Batch requests | 50% |

### Team Setup Checklist

- [ ] Define approved tools list
- [ ] Create governance policy
- [ ] Set up shared configuration files
- [ ] Train team on best practices
- [ ] Establish code review process
- [ ] Configure security controls
- [ ] Set up cost monitoring
- [ ] Create shared prompt library

---

## Related Documents

- [agent-prompting-guide.md](agent-prompting-guide.md) - Detailed prompting techniques
- [patterns-and-antipatterns.md](patterns-and-antipatterns.md) - Common failures
- [security-essentials.md](security-essentials.md) - Security implementation
- [api-optimization-guide.md](api-optimization-guide.md) - Cost optimization

---

**Document Version:** 1.0
**Last Updated:** December 2025
**Sources:** JetBrains, CodeRabbit, Qodo, Cloud Security Alliance, OX Security, Cognition, Fortune, 30+ industry reports
