# Developer Productivity with AI Coding Assistants

**Best practices for Cursor, Claude Code, Windsurf, and autonomous agents**

**Last Updated:** 2025-12-26 | Version 2.0

**New in v2.0:** RIPER Framework, Spec-Driven Development, Context Engineering, Team Workflows from Martin Fowler

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Cursor IDE Best Practices](#1-cursor-ide-best-practices)
3. [Claude Code CLI Best Practices](#2-claude-code-cli-best-practices)
4. [Windsurf & Cascade](#3-windsurf--cascade)
5. [Autonomous Agents (Devin)](#4-autonomous-agents-devin)
6. [Production Workflows](#5-production-workflows)
7. [Team Collaboration](#6-team-collaboration)
8. [Performance Optimization](#7-performance-optimization)
9. [Security Considerations](#8-security-considerations)
10. [Common Pitfalls](#9-common-pitfalls)
11. [Spec-Driven Development (SDD)](#11-spec-driven-development-sdd)
12. [Context Engineering](#12-context-engineering)
13. [Quick Reference](#13-quick-reference)

---

## Executive Summary

### Market Context (December 2025)

| Metric | Value | Source |
|--------|-------|--------|
| Developers using AI coding assistants weekly | **82%** | [Stack Overflow 2025](https://visualstudiomagazine.com/articles/2025/08/01/stack-overflow-dev-survey-visual-studio-vs-code-hold-of-ai-ides-to-remain-on-top.aspx) |
| All code globally that is AI-generated | **41%** | GitHub |
| Google's new code generated by AI | **25%** | Google |
| AI coding tools market size | **$4.8B** (projected $17.2B by 2030) | [CB Insights](https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/) |
| Individual productivity increase | 20-40% | Industry Research |
| AI code containing security vulnerabilities | **45%** | Veracode 2025 |
| AI-generated code with design flaws | 62% | Cloud Security Alliance |
| Java AI code with security failures | **70%** | Veracode |
| AI PRs with more defects than human | 70% more | CodeRabbit |
| Developer positive sentiment (down from 70%+) | **60%** | Stack Overflow |

### Market Leaders (December 2025)

| Tool | Market Position | Notes |
|------|-----------------|-------|
| **ChatGPT** | 82% usage | Primary entry point for developers |
| **GitHub Copilot** | 68% usage | Leading code completion |
| **Cursor** | 18% usage (AI IDEs) | $9.9B valuation; 45% of Y Combinator companies |
| **Claude Code** | 10% usage (AI IDEs) | $1B+ ARR threshold crossed |
| **Windsurf** | 5% usage (AI IDEs) | Growing flow-aware IDE |

> **Market Shift (August 2025):** Cursor overtook GitHub Copilot in organizational adoption rates (43% vs 37%).

### Key Insight

> "AI tools demonstrably increase individual developer output by 20-40%, but realizing company-level delivery gains requires systematic process changes, rigorous governance, and strategic investments in team capabilities."

### Tool Comparison

| Tool | Type | Best For | Learning Curve |
|------|------|----------|----------------|
| **Cursor** | Full IDE | Complex projects, multi-file editing | Medium |
| **Claude Code** | CLI | Terminal-first workflows, MCP integration | Medium |
| **Windsurf/Cascade** | Full IDE | Flow-aware development, agentic tasks | Low-Medium |
| **GitHub Copilot** | Extension | Quick completions, existing IDE users | Low |
| **Devin** | Autonomous | Well-scoped tasks, parallel execution | Medium |

---

## 1. Cursor IDE Best Practices

### Configuration Architecture

Cursor uses a three-tier configuration hierarchy:

```
┌─────────────────────────────────────────────────────────────────┐
│                    CONFIGURATION HIERARCHY                       │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  GLOBAL RULES (~/.cursor/rules/)                           ││
│  │  Personal preferences across all projects                   ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  PROJECT RULES (.cursor/rules/)                            ││
│  │  Team-shared patterns, committed to repo                    ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  LOCAL RULES (.cursor/rules/.local/)                       ││
│  │  Personal experiments, not committed                        ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘
```

### .mdc File Format (Replaces .cursorrules)

The new `.mdc` format in `.cursor/rules/` directory:

```yaml
---
name: "TypeScript Patterns"
version: "1.0"
globs: ["src/**/*.ts", "src/**/*.tsx"]
alwaysInclude: false
autoAttach: true
---

# TypeScript Development Patterns

## Naming Conventions
- Use PascalCase for components and types
- Use camelCase for functions and variables
- Use UPPER_SNAKE_CASE for constants

## Error Handling
Always use custom error types:

```typescript
class DomainError extends Error {
  constructor(
    message: string,
    public readonly code: string,
    public readonly cause?: Error
  ) {
    super(message);
    this.name = 'DomainError';
  }
}
```

## Why These Patterns
We've had production incidents from type confusion. Strict TypeScript
prevents similar issues. Always enable strict mode.
```

### Rule File Organization

**Recommended Structure:**

```
.cursor/
└── rules/
    ├── code-style.mdc       # Formatting, naming conventions
    ├── architecture.mdc      # Project structure, patterns
    ├── testing.mdc          # Test conventions, frameworks
    ├── security.mdc         # Security requirements
    ├── api-standards.mdc    # API design patterns
    └── .local/
        └── experiments.mdc  # Personal experiments
```

### Cursor Modes

| Mode | Access | Best For | Behavior |
|------|--------|----------|----------|
| **Chat** | Cmd+L | Understanding, explanations | Read-only, explains |
| **Composer** | Cmd+I | Multi-file implementation | Direct modification |
| **Agent** | Cmd+I → Agent | Complex workflows, terminal | Autonomous execution |

**Mode Selection Guide:**

```
What do you need?
│
├── Understanding existing code → Chat (Cmd+L)
├── Implementing known feature → Composer (Cmd+I)
├── Complex task with terminal → Agent Mode
└── Quick inline edit → Inline Edit (Cmd+K)
```

### Essential Keyboard Shortcuts

| Action | Mac | Windows | Use Case |
|--------|-----|---------|----------|
| Open Chat | Cmd+L | Ctrl+L | Ask questions |
| Open Composer | Cmd+I | Ctrl+I | Multi-file changes |
| Inline Edit | Cmd+K | Ctrl+K | Modify selected code |
| Toggle Terminal | Cmd+` | Ctrl+` | Run commands |
| New Chat Tab | Cmd+T | Ctrl+T | Separate conversation |
| Command Palette | Cmd+Shift+P | Ctrl+Shift+P | All commands |
| File Reference | @filename | @filename | Add specific file to context |
| Folder Reference | @folder | @folder | Add directory to context |
| Codebase Query | @codebase | @codebase | Search entire project |

### Context Management

**Problem:** Context window (~200K tokens) fills quickly with large codebases.

**Solution:** Explicit context inclusion:

```
Best Practice:
@src/services/auth.ts           # Specific file
@src/components/Login.tsx       # Related component
@instructions.md                # Project context

Avoid:
@codebase                       # Unless needed
Including entire directories     # Too much noise
```

**Create Context Files:**

```markdown
<!-- instructions.md -->
# Project Overview
E-commerce platform with React frontend, Node backend.

# Architecture
- Frontend: React + TypeScript + TanStack Query
- Backend: Express + PostgreSQL + Redis
- Auth: JWT with refresh tokens

# Key Patterns
- All API calls through /src/api/client.ts
- State management via React Query (no Redux)
- Error boundaries around all route components
```

### Agent Mode Best Practices

**When to Use Agent Mode:**

| Scenario | Use Agent Mode? |
|----------|-----------------|
| Simple code completion | No → Use inline edit |
| Multi-step implementation | Yes |
| Requires running tests | Yes |
| Needs terminal commands | Yes |
| Understanding code only | No → Use Chat |

**Agent Mode Workflow:**

```
1. Clear, specific request
   "Implement user authentication with JWT, include tests"

2. Let agent work
   - Creates files
   - Runs commands
   - Checks results

3. Review output
   - Accept/reject changes
   - Request adjustments

4. Verify
   - Run tests yourself
   - Check edge cases
```

### Cursor 2.0: Agent-Centric Development

**Released:** October 29, 2025 — Major update pairing purpose-built Composer model with agent-centered interface.

**Key Features:**

| Feature | Description |
|---------|-------------|
| **Parallel Agents** | Run up to **8 coding agents simultaneously** in isolated environments |
| **Background Agents** | Work on separate branches, can open PRs; "AI pair programmers in isolated Ubuntu VMs" |
| **Composer Model** | 4x faster than similarly intelligent models; completes most tasks in <30s |
| **Browser Tool** | Agents can test web applications, capture screenshots, identify issues |
| **Voice Mode** | Speech-to-text for controlling agents with custom trigger keywords |

```
┌─────────────────────────────────────────────────────────────────┐
│                 PARALLEL AGENTS ARCHITECTURE                     │
│                                                                  │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │    Agent 1      │  │    Agent 2      │  │    Agent 3      │ │
│  │                 │  │                 │  │                 │ │
│  │  worktree-1/    │  │  worktree-2/    │  │  worktree-3/    │ │
│  │  Feature A      │  │  Feature B      │  │  Bug Fix C      │ │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘ │
│           │                    │                    │           │
│           └────────────────────┼────────────────────┘           │
│                                ▼                                │
│                        Main Repository                          │
│                       (shared .git dir)                         │
└─────────────────────────────────────────────────────────────────┘
```

**Multi-Model Comparison:**
Run the same task through multiple models (GPT-5, Claude Sonnet 4.5, Composer) to compare approaches—GPT-5 prioritizes error handling, Claude focuses on clean structure, Composer aims for minimal changes.

**Background Agents:**
- Spin up from Cursor, Slack, or web/mobile
- Work on separate branches
- Can open PRs for review
- 99.9% reliability for cloud agents

**Setup:**
```bash
# Create worktree for parallel work
git worktree add ../feature-auth -b feature/auth

# Agent works in isolated copy
cd ../feature-auth
# (Start Cursor agent here)
```

**Best Practices:**
- Use for independent, parallelizable tasks
- Merge completed worktrees promptly
- Clean up unused worktrees regularly

### RIPER Framework for Structured Development

> **RIPER** = **R**esearch → **I**nnovate → **P**lan → **E**xecute → **R**eview

The RIPER framework provides a disciplined workflow preventing unintended code modifications while maintaining perfect context across coding sessions.

**The 5 RIPER Modes:**

| Mode | Purpose | Output | Rule |
|------|---------|--------|------|
| **Research** | Understand existing code | Analysis only | NO code changes |
| **Innovate** | Brainstorm solutions | Options list | NO code changes |
| **Plan** | Design implementation | Detailed plan | NO code changes |
| **Execute** | Implement changes | Working code | Only planned changes |
| **Review** | Verify results | Validation report | NO new changes |

```
┌─────────────────────────────────────────────────────────────────┐
│                    RIPER WORKFLOW                                │
│                                                                  │
│  ┌──────────┐   ┌──────────┐   ┌──────────┐                    │
│  │ RESEARCH │ → │ INNOVATE │ → │   PLAN   │                    │
│  │          │   │          │   │          │                    │
│  │ Explore  │   │ Generate │   │ Document │                    │
│  │ codebase │   │ options  │   │ approach │                    │
│  └──────────┘   └──────────┘   └──────────┘                    │
│       │                              │                          │
│       │                              ▼                          │
│  ┌──────────┐               ┌──────────┐                       │
│  │  REVIEW  │ ←──────────── │ EXECUTE  │                       │
│  │          │               │          │                       │
│  │ Validate │               │ Implement│                       │
│  │ changes  │               │ plan     │                       │
│  └──────────┘               └──────────┘                       │
└─────────────────────────────────────────────────────────────────┘
```

**Primary Directive:** ZERO UNAUTHORIZED CHANGES — The AI must never modify code outside the approved plan or protocol phase.

**Setting Up RIPER:**

1. **Add rules to Cursor:**
```markdown
<!-- .cursor/rules/riper.mdc -->
---
name: "RIPER Protocol"
version: "1.0"
globs: ["**/*"]
alwaysInclude: true
---

# RIPER-5 Protocol

## Modes
You must always declare your current mode before any action:
- MODE: RESEARCH - Gathering information, no code changes
- MODE: INNOVATE - Brainstorming approaches, no code changes
- MODE: PLAN - Creating implementation plan, no code changes
- MODE: EXECUTE - Implementing approved plan only
- MODE: REVIEW - Validating implementation, no changes

## Rules
1. Start every response with mode declaration
2. Never skip modes (RESEARCH before INNOVATE before PLAN)
3. No code modifications in Research/Innovate/Plan modes
4. Execute mode: only implement what was approved in Plan
5. Review mode: validate only, no new changes
```

2. **Initialize Memory Bank:**
Create persistent context files:
```
.cursor/memory/
├── activeContext.md    # Current session state
├── progress.md         # Completed work tracking
├── decisions.md        # Architectural decisions
└── projectBrief.md     # Project overview
```

**Memory Bank Template:**
```markdown
<!-- .cursor/memory/activeContext.md -->
# Active Context

## Current Phase: [RESEARCH/INNOVATE/PLAN/EXECUTE/REVIEW]

## Current Focus
[What we're working on]

## Open Questions
- [ ] Question 1
- [ ] Question 2

## Recent Decisions
- Decision 1: [Reasoning]

## Session Notes
[Learnings from current session]
```

**RIPER Best Practices:**
- **Always complete Research** before proposing changes
- **Document innovations** even if not selected
- **Plan approval required** before any Execute mode
- **Review every change** before marking complete
- **Persist context** in Memory Bank files

**When to Use RIPER:**
| Scenario | Use RIPER? |
|----------|------------|
| Simple bug fix | Optional (can skip to Execute) |
| New feature | Yes (full cycle) |
| Refactoring | Yes (full cycle) |
| Quick question | No (use Chat mode) |
| Understanding code | RESEARCH mode only |

**Source:** [CursorRIPER Framework](https://github.com/johnpeterman72/CursorRIPER), [Cursor Community Forum](https://forum.cursor.com/t/i-created-an-amazing-mode-called-riper-5-mode-fixes-claude-3-7-drastically/65516)

---

## 2. Claude Code CLI Best Practices

### CLAUDE.md Architecture

> **"CLAUDE.md is the single most impactful optimization for Claude Code"** — Anthropic Best Practices

Claude Code uses markdown files for persistent memory:

```
┌─────────────────────────────────────────────────────────────────┐
│                    CLAUDE.md HIERARCHY                           │
│                                                                  │
│  ~/.claude/CLAUDE.md                                            │
│  └── Personal preferences across all projects                   │
│                                                                  │
│  ~/project/CLAUDE.md                                            │
│  └── Team-shared, version controlled                            │
│                                                                  │
│  ~/project/frontend/CLAUDE.md                                   │
│  └── Subsystem-specific context                                 │
│                                                                  │
│  ~/project/backend/CLAUDE.md                                    │
│  └── Subsystem-specific context                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Key Difference from Cursor:** CLAUDE.md files are:
- Version controlled (shared with team)
- Loaded automatically at session start
- Updated via `#` command during conversation

### CLAUDE.md Template

```markdown
# Project: E-Commerce Platform

## Overview
Modern e-commerce platform serving 50K daily users.
React frontend, Node.js backend, PostgreSQL database.

## Architecture
- Frontend: `src/web/` - React 18, TypeScript, Vite
- Backend: `src/api/` - Express, Prisma ORM
- Shared: `src/shared/` - Types, utilities

## Coding Standards

### TypeScript
- Strict mode enabled
- Prefer `interface` over `type` for objects
- Always define return types for functions

### Error Handling
```typescript
// Always use Result pattern for fallible operations
type Result<T, E = Error> = { ok: true; value: T } | { ok: false; error: E };
```

### Testing
- Jest for unit tests, Playwright for E2E
- Minimum 80% coverage for new code
- Co-locate tests with source: `foo.ts` → `foo.test.ts`

## Common Commands
```bash
npm run dev          # Start development
npm run test         # Run tests
npm run lint         # Lint check
npm run build        # Production build
```

## Key Files
- `src/api/routes/` - API endpoints
- `src/web/pages/` - Page components
- `prisma/schema.prisma` - Database schema

## Recent Decisions
- 2025-12-20: Switched from Redux to TanStack Query
- 2025-12-15: Added rate limiting to all endpoints
```

### Updating CLAUDE.md During Conversation

Use `#` command to add learnings:

```
User: # Always wrap API responses in try-catch blocks

Claude: I'll add that to your CLAUDE.md file.
```

This creates persistent memory that survives sessions.

### Custom Slash Commands

Create reusable commands in `.claude/commands/`:

```markdown
<!-- .claude/commands/review.md -->
# Code Review Command

Perform a comprehensive code review:

1. Check for security vulnerabilities
2. Verify error handling
3. Ensure test coverage
4. Review naming conventions
5. Check for performance issues

Focus on files changed in the current branch:
```bash
git diff --name-only main
```

Provide specific, actionable feedback.
```

**Usage:** `/review` in conversation

### Claude Code Hooks

Configure automated actions in settings:

```json
{
  "hooks": {
    "post-file-write": [
      "prettier --write {file}",
      "eslint --fix {file}"
    ],
    "pre-commit": [
      "npm run lint",
      "npm run test"
    ]
  }
}
```

### MCP Server Integration

Model Context Protocol enables tool integration:

```json
{
  "mcpServers": {
    "database": {
      "command": "mcp-server-postgres",
      "args": ["--connection-string", "$DATABASE_URL"]
    },
    "github": {
      "command": "mcp-server-github",
      "env": {
        "GITHUB_TOKEN": "$GITHUB_TOKEN"
      }
    }
  }
}
```

**Configuration Locations:**
| Location | Scope | Use Case |
|----------|-------|----------|
| `.mcp.json` (project root) | Project | Team-shared, version controlled |
| `.claude/settings.local.json` | Project | Personal project-specific |
| `~/.claude/settings.local.json` | User | Personal tooling across projects |

**Popular MCP Servers (2025):**
- `mcp-server-filesystem` - File operations
- `mcp-server-github` - GitHub integration
- `mcp-server-postgres` - Database queries
- `mcp-server-slack` - Slack messaging
- `mcp-server-browserbase` - Web scraping
- `Sequential Thinking MCP` - Enhanced problem-solving
- `Puppeteer MCP` - Web automation

**⚠️ Security Warning:** Use third-party MCP servers at your own risk. Be especially careful with servers that fetch untrusted content (prompt injection risk).

**Debugging MCP:**
```bash
# Launch with debug flag
claude --mcp-debug

# Check server status
/mcp  # Shows "connected" or "failed" for each server
```

### Session Memory Management

Claude Code automatically saves session memory:

```
Location: ~/.claude/session-memory/[session-id].md
Triggers:
- After ~10,000 tokens
- Every ~5,000 tokens thereafter
- After every 3 tool calls
```

**Review session memory:**
```bash
ls ~/.claude/session-memory/
cat ~/.claude/session-memory/latest.md
```

---

## 3. Windsurf & Cascade

### Cascade's Flow Awareness

Cascade tracks all developer actions:
- File edits
- Terminal commands
- Clipboard activity
- Conversation history

**Benefit:** Reduced context re-explanation

```
Traditional AI:
"I modified auth.ts to add JWT validation, then ran tests
which failed because of missing mock. I added the mock..."

Cascade:
"Continue my work"
(Cascade already knows what you did)
```

### Cascade Modes

| Mode | Purpose | File Modification |
|------|---------|-------------------|
| **Write** | Create/modify code | Yes |
| **Chat** | Questions, explanations | No |

**Switch modes:** Cmd+L or click interface icon

### .codeiumignore Configuration

Protect sensitive or irrelevant files:

```gitignore
# .codeiumignore

# Sensitive files
.env*
*.pem
credentials/

# Large irrelevant directories
node_modules/
dist/
.next/

# Legacy code (don't touch)
legacy/
deprecated/
```

**Global ignore:** `~/.codeium/.codeiumignore`

### Cascade Workflow Tips

**Tool Call Limits:**
- Up to 25 tool calls per prompt
- Type `continue` to resume after limit
- Queue messages while working

**Best Practices:**

```
1. Start with Chat mode to understand
2. Switch to Write mode to implement
3. Let Cascade detect your environment
4. Use `continue` for long workflows
```

### Windsurf vs Cursor

| Feature | Windsurf | Cursor |
|---------|----------|--------|
| Flow awareness | Built-in tracking | Manual context |
| Configuration | .codeiumignore | .mdc rules |
| Modes | Write/Chat | Chat/Composer/Agent |
| Tool calls | 25 per prompt | Varies |
| Auto-detection | Strong (deps, build) | Good |
| Parallel agents | Coming (2.0) | Yes (git worktrees) |

### Cascade 2.0 (Coming Soon)

Windsurf is developing **Cascade 2.0** with:
- Multi-agent collaboration capabilities
- Enhanced flow awareness across agents
- Improved parallel task execution

**Current Status:** Beta expected Q1 2025

---

## 4. Autonomous Agents (Devin)

### Appropriate Use Cases

**Devin excels at:**
- Well-scoped tasks (4-8 hour junior engineer work)
- Clear acceptance criteria
- Verifiable outcomes

| Good Fit | Poor Fit |
|----------|----------|
| Code reviews (first pass) | Ambiguous requirements |
| Unit test writing | Architectural decisions |
| Codebase migrations | Complex debugging |
| Security vulnerability fixes | Creative problem-solving |
| Documentation generation | Mid-task requirement changes |

### Performance Benchmarks (December 2025)

| Metric | Current | Previous Year | Improvement |
|--------|---------|---------------|-------------|
| Problem-solving speed | 4x faster | Baseline | 4x |
| Resource efficiency | 2x better | Baseline | 2x |
| PR merge rate | **67%** | 34% | ~2x |
| Security fix time | 1.5 min | 30 min (human) | **20x efficiency** |
| SWE-bench (unassisted) | **13.86%** | 1.96% (prior best) | 7x |
| Total PRs merged | Hundreds of thousands | N/A | Production scale |

**Enterprise Adoption:**
- Goldman Sachs, Santander, Nubank among users
- [Bilt](https://cognition.ai/blog/devin-annual-performance-review-2025): 800+ PRs merged, >50% acceptance rate
- [Ramp](https://cognition.ai/blog/devin-annual-performance-review-2025): Up to 80 PRs/week for tech debt cleanup
- [Nubank](https://cognition.ai/blog/devin-annual-performance-review-2025): 12x efficiency improvement, 20x cost savings

**Key Achievement:** Devin's PR merge rate doubled from 34% to 67% in 2025, making it viable for production workflows with appropriate oversight.

**Reality Check:** Independent testing (Answer.AI) found 15% success rate (3/20 tasks). Devin is "senior-level at codebase understanding but junior at execution."

### Fleet Deployment Pattern

Run multiple Devin instances in parallel:

```
┌─────────────────────────────────────────────────────────────────┐
│                     FLEET ARCHITECTURE                           │
│                                                                  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │   Devin 1   │  │   Devin 2   │  │   Devin 3   │              │
│  │             │  │             │  │             │              │
│  │ Migrate     │  │ Migrate     │  │ Migrate     │              │
│  │ Module A    │  │ Module B    │  │ Module C    │              │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘              │
│         │                │                │                      │
│         └────────────────┼────────────────┘                      │
│                          ▼                                       │
│                   Code Review                                    │
│                   (Human or AI)                                  │
└─────────────────────────────────────────────────────────────────┘
```

**Use Case:** Angular → React migration across 50 repositories

### Oversight Strategies

**Sampling-Based Review:**
- Don't review every change
- Sample 10-20% for detailed review
- Full review for security-sensitive changes

**Automated Guardrails:**
- Required test coverage thresholds
- Static analysis gates
- Security scanning

### When NOT to Use Autonomous Agents

| Scenario | Why |
|----------|-----|
| Requirements unclear | Can't iterate mid-task effectively |
| Novel architecture | Needs human judgment |
| Performance-critical | May not optimize well |
| Security-critical | Requires expert review |

---

## 5. Production Workflows

### Testing Strategy for AI-Generated Code

**Traditional TDD (Red-Green-Refactor) is inefficient with AI.**

**Recommended: Comprehensive Test-First Approach:**

```
1. Generate ALL tests for the feature first
   "Generate comprehensive tests for user authentication:
    - Happy path: valid login
    - Edge cases: expired tokens, invalid credentials
    - Error handling: network failures, rate limiting"

2. Implement to pass all tests
   "Implement authentication to pass all tests above"

3. Review and iterate
   - Fix any failing tests
   - Add missed edge cases
```

### Acceptance Test-Driven Development (ATDD)

```markdown
## Feature: User Login

### Scenario: Successful login with valid credentials
Given a registered user with email "user@example.com"
When they submit valid password
Then they receive a JWT token
And the token expires in 24 hours

### Scenario: Failed login with invalid password
Given a registered user
When they submit wrong password
Then they receive 401 error
And no token is issued
And rate limit counter increases

### Scenario: Account lockout after failures
Given a user with 4 failed attempts
When they fail a 5th time
Then account is locked for 15 minutes
And notification email is sent
```

Generate tests from these scenarios, then implement.

### Code Review for AI Code

**Multi-Stage Validation:**

```
┌─────────────────────────────────────────────────────────────────┐
│                    CODE REVIEW PIPELINE                          │
│                                                                  │
│  Stage 1: AUTOMATED CHECKS                                       │
│  ├── Linting (ESLint, Prettier)                                 │
│  ├── Type checking (TypeScript)                                 │
│  ├── Static analysis (SonarQube)                                │
│  └── Security scanning (Snyk, OWASP)                            │
│                                                                  │
│  Stage 2: AI CODE REVIEW                                        │
│  ├── Logic error detection                                       │
│  ├── Pattern compliance                                          │
│  └── Architectural fit                                           │
│                                                                  │
│  Stage 3: HUMAN REVIEW (if passed above)                        │
│  ├── Business logic correctness                                  │
│  ├── Architectural decisions                                     │
│  └── Security-sensitive areas                                    │
└─────────────────────────────────────────────────────────────────┘
```

**Key Statistic:** 62% of AI-generated code has design flaws or security vulnerabilities.

### CI/CD Integration

```yaml
# .github/workflows/ai-code-check.yml
name: AI Code Quality Check

on: [pull_request]

jobs:
  ai-review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Stage 1: Automated checks
      - name: Lint
        run: npm run lint

      - name: Type Check
        run: npm run typecheck

      - name: Security Scan
        run: npm audit --audit-level=high

      # Stage 2: AI review (optional)
      - name: AI Code Review
        uses: coderabbit-ai/review@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}

      # Stage 3: Quality gates
      - name: Test Coverage
        run: npm run test -- --coverage

      - name: Coverage Threshold
        run: |
          coverage=$(jq '.total.lines.pct' coverage/coverage-summary.json)
          if (( $(echo "$coverage < 80" | bc -l) )); then
            echo "Coverage $coverage% is below 80% threshold"
            exit 1
          fi
```

---

## 6. Team Collaboration

### AI Does NOT Replace Pair Programming

> **"Framing coding assistants as pair programmers ignores one of the key benefits of pairing: to make the team, not just individual contributors, better."** — [Thoughtworks Technology Radar](https://www.thoughtworks.com/en-us/radar/techniques/replacing-pair-programming-with-ai)

**What AI Assistants Provide:**
- Getting unstuck on technical problems
- Learning about new technologies
- Onboarding to codebases
- Accelerating tactical coding work

**What AI Assistants DON'T Provide:**
- Keeping work-in-progress low
- Reducing handoffs and relearning
- Enabling continuous integration through collaboration
- Improving collective code ownership
- Knowledge transfer between team members

**The 2025 Shift: From "Vibe Coding" to "Context Engineering"**

```
┌─────────────────────────────────────────────────────────────────┐
│                 AI CODING EVOLUTION (2025)                       │
│                                                                  │
│  Early 2025: VIBE CODING                                        │
│  ├── "Just tell the AI what you want"                           │
│  ├── Trial and error with prompts                               │
│  └── 25% Y Combinator: 95% AI-generated code                   │
│                                                                  │
│  Late 2025: CONTEXT ENGINEERING                                  │
│  ├── Systematic context management                              │
│  ├── Spec-driven development                                    │
│  └── Human developers remain absolutely critical                │
└─────────────────────────────────────────────────────────────────┘
```

**Source:** [MIT Technology Review: From Vibe Coding to Context Engineering](https://www.technologyreview.com/2025/11/05/1127477/from-vibe-coding-to-context-engineering-2025-in-software-development/)

### Reference Application Anchoring

**Thoughtworks Practice:** Anchor coding agents to a reference application as contextual ground truth.

```
┌─────────────────────────────────────────────────────────────────┐
│                REFERENCE APPLICATION PATTERN                     │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │              REFERENCE APPLICATION                         │  │
│  │  A working implementation demonstrating:                   │  │
│  │  • Architectural patterns                                  │  │
│  │  • Coding standards                                        │  │
│  │  • Integration patterns                                    │  │
│  │  • Test structure                                          │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              │                                   │
│                              ▼                                   │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │              AI CODING AGENT                               │  │
│  │  "Follow the patterns in the reference app when           │  │
│  │   implementing new features in the target codebase"       │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              │                                   │
│                              ▼                                   │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │              TARGET CODEBASE                               │  │
│  │  New code follows established patterns automatically       │  │
│  └───────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

**Implementation:**
```markdown
<!-- CLAUDE.md for reference anchoring -->
## Reference Application

When implementing new features, follow patterns from:
- Reference app: /reference-app/
- Key patterns to follow:
  - API structure: /reference-app/src/api/
  - Component structure: /reference-app/src/components/
  - Test patterns: /reference-app/tests/

## Pattern Requirements
Always check reference app before implementing:
1. How similar features are structured
2. Error handling patterns used
3. Test coverage expectations
```

**Source:** [Thoughtworks: Exploring GenAI](https://martinfowler.com/articles/exploring-gen-ai.html)

### Multi-Agent Team Workflows

**Team of Agents Pattern:**
Use multiple specialized agents to reduce context burden on any single agent.

| Agent | Responsibility | Context |
|-------|---------------|---------|
| **Research Agent** | Codebase exploration | File index, search tools |
| **Implementation Agent** | Writing code | Spec + relevant files |
| **Test Agent** | Validation | Test frameworks, coverage |
| **Review Agent** | Code review | Standards, patterns |

**Human-AI Pairing Best Practices:**

```
┌─────────────────────────────────────────────────────────────────┐
│               HUMAN-AI COLLABORATION PATTERNS                    │
│                                                                  │
│  Pattern 1: PLAN → ACT → REFLECT                                 │
│  ├── Human writes clear plan/spec                               │
│  ├── AI implements according to plan                            │
│  └── Human reviews, AI reflects on feedback                     │
│                                                                  │
│  Pattern 2: VISIBILITY + CONTROL                                 │
│  ├── AI shows what it's doing at each step                      │
│  ├── Human can interrupt and redirect                           │
│  └── AI knows when/how to ask for help                          │
│                                                                  │
│  Pattern 3: SHARED KNOWLEDGE BASE                                │
│  ├── Team standards in CLAUDE.md/.mdc                           │
│  ├── Coding conventions documented                              │
│  └── Common troubleshooting steps captured                      │
└─────────────────────────────────────────────────────────────────┘
```

**Productivity Metrics (2025 Research):**

| Metric | Impact | Source |
|--------|--------|--------|
| Routine coding tasks | +30-50% productivity | Industry Research |
| PR cycle time | -45% reduction | UiPath |
| Complex problem-solving | Variable | Thoughtworks |
| Code quality without oversight | Potentially negative | Multiple |

### Treating AI as Junior Developer

> **"No matter how advanced your coding agent is, always perform a human review. Treat the agent as a capable junior developer—efficient, but always in need of supervision and validation."** — Industry Best Practice

**Junior Developer Mental Model:**
- **Capable:** Can write functional code
- **Fast:** Completes tasks quickly
- **Eager:** Will attempt anything asked
- **Needs guidance:** May miss edge cases, security issues
- **Requires review:** All output needs verification

**Review Checklist for AI-Generated Code:**
```markdown
## AI Code Review Checklist

### Logic & Correctness
- [ ] Does the code do what was requested?
- [ ] Are edge cases handled?
- [ ] Is error handling appropriate?

### Security
- [ ] No injection vulnerabilities?
- [ ] Secrets handled properly?
- [ ] Input validation present?

### Architecture
- [ ] Follows project patterns?
- [ ] No unnecessary complexity?
- [ ] Appropriate abstractions?

### Testing
- [ ] Tests cover happy path?
- [ ] Tests cover error cases?
- [ ] Integration tests if needed?
```

### Knowledge Sharing with AI Tools

**Weekly AI Tools Sync (15 min):**
1. What prompts worked well this week?
2. What AI mistakes did we catch?
3. New patterns to share?
4. Updates to shared CLAUDE.md/.mdc?

**Shared Prompt Library:**

```
.team/prompts/
├── new-endpoint.md      # Standard API endpoint pattern
├── component.md         # React component template
├── migration.md         # Database migration pattern
├── security-review.md   # Security audit checklist
└── refactor.md          # Refactoring workflow
```

**Onboarding New Team Members:**

```markdown
## AI Tools Onboarding Checklist

### Day 1
- [ ] Access to approved AI tools (Cursor/Claude Code/Copilot)
- [ ] Walk through CLAUDE.md / .mdc files
- [ ] Review team prompt library

### Week 1
- [ ] Shadow experienced AI user
- [ ] Complete L1 training (basics)
- [ ] First AI-assisted PR (with review)

### Week 2-4
- [ ] L2 training (intermediate)
- [ ] Independent AI usage with review
- [ ] Contribute to shared prompt library

### Month 2+
- [ ] L3 training (advanced)
- [ ] Help onboard next team member
- [ ] Propose improvements to AI workflows
```

**Source:** [Thoughtworks: AI-Assisted Coding](https://www.thoughtworks.com/en-us/insights/podcasts/technology-podcasts/ai-assisted-coding-experiences-perspectives), [UiPath: Best Practices](https://www.uipath.com/blog/ai/agent-builder-best-practices)

### AI Tool Governance Policy Template

```markdown
# AI Coding Tools Policy

## Approved Tools
- Cursor (Enterprise license)
- Claude Code (Team license)
- GitHub Copilot (Enterprise)

## Prohibited Tools
- Consumer ChatGPT (code exposure risk)
- Unvetted browser extensions
- Tools without SOC2 compliance

## Usage Guidelines

### Must Do
1. Review all AI-generated code before committing
2. Run security scans on AI changes
3. Disclose AI usage in PR descriptions
4. Use approved context/rules files

### Must Not
1. Paste proprietary code into consumer AI tools
2. Accept AI suggestions without review
3. Disable security scans for AI code
4. Share API keys or credentials with AI

## Code Attribution
Include in commit message when AI-assisted:
```
feat: Add user authentication

AI-assisted: Cursor Agent Mode
Human review: [Your name]
```

## Incident Reporting
If AI tool produces concerning output:
1. Document the prompt and response
2. Report to security team
3. Do not commit the code
```

### Shared Prompt Library

Create team-wide prompt patterns:

```markdown
<!-- .team/prompts/new-api-endpoint.md -->
# New API Endpoint Prompt

Create a new REST API endpoint with the following requirements:

## Endpoint Details
- Method: [GET/POST/PUT/DELETE]
- Path: [/api/v1/...]
- Description: [What this endpoint does]

## Requirements
1. Use our standard Express middleware chain
2. Validate input with Zod schemas
3. Return standard response format:
   ```json
   { "success": true, "data": {...} }
   ```
4. Include error handling with our error codes
5. Add OpenAPI documentation comments
6. Write unit tests achieving 80% coverage

## Reference Files
@src/api/middleware/auth.ts
@src/api/utils/response.ts
@src/api/schemas/index.ts
```

### Team Training Program

| Level | Content | Duration | Audience |
|-------|---------|----------|----------|
| **L1: Basics** | What AI tools do, basic usage | 2 hours | All devs |
| **L2: Intermediate** | Context management, modes, shortcuts | 4 hours | Regular users |
| **L3: Advanced** | Prompt engineering, meta-prompting | 8 hours | Power users |
| **L4: Admin** | Security, governance, cost management | 4 hours | Team leads |

### Knowledge Sharing Practices

**Weekly Sync (15 min):**
- What prompts worked well?
- What AI mistakes did we catch?
- New patterns to share?

**Slack Channel:** `#ai-tools-tips`
- Share successful prompts
- Report issues
- Discuss new techniques

**Documentation:**
- Update shared rules files monthly
- Document AI limitations specific to your codebase
- Track cost per developer/project

---

## 7. Performance Optimization

### Context Caching by Provider

| Provider | Cache Duration | Minimum Size | Cost Reduction |
|----------|---------------|--------------|----------------|
| OpenAI | 1 hour | Auto | 50-90% |
| Anthropic | 5-60 min | Varies | 50-90% |
| Google | 24 hours | 32K tokens | 50-90% |

**Caching Strategy:**

```
┌─────────────────────────────────────────────────────────────────┐
│                     OPTIMAL PROMPT STRUCTURE                     │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  STATIC (Cached)                                           ││
│  │  System instructions, examples, context                     ││
│  └─────────────────────────────────────────────────────────────┘│
│                              │                                   │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  DYNAMIC (Not Cached)                                       ││
│  │  User query, variable content                               ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘

WRONG:
[User query] + [System instructions] + [Examples]

RIGHT:
[System instructions] + [Examples] + [User query]
```

### Context Management Strategies

| Strategy | Cost Reduction | Success Rate Impact |
|----------|---------------|---------------------|
| Observation masking | 50%+ | Neutral |
| LLM summarization | 40-50% | Slight decrease |
| Hybrid (mask + summarize) | 55-60% | +2.6% |

**Observation Masking:**
```
Replace: "File auth.ts was read. Contents: [2000 tokens of code]..."
With: "File auth.ts was read at step 47"
```

### Token Optimization Techniques

| Technique | Savings | Effort |
|-----------|---------|--------|
| Concise prompting | 20-30% | Low |
| Remove redundant context | 30-40% | Low |
| Use model-appropriate tier | 40-60% | Low |
| Implement RAG | 70% | Medium |
| Batch non-urgent requests | 50% discount | Medium |

### Cost Monitoring

Track per-developer and per-project:

```javascript
// Example cost tracking
const costTracker = {
  logUsage(developer, project, tokens, model) {
    const cost = this.calculateCost(tokens, model);
    this.db.insert({
      timestamp: new Date(),
      developer,
      project,
      inputTokens: tokens.input,
      outputTokens: tokens.output,
      model,
      cost
    });
  },

  calculateCost(tokens, model) {
    const rates = {
      'gpt-4o': { input: 0.0025, output: 0.01 },
      'gpt-4o-mini': { input: 0.00015, output: 0.0006 },
      'claude-sonnet': { input: 0.003, output: 0.015 }
    };
    const rate = rates[model];
    return (tokens.input * rate.input + tokens.output * rate.output) / 1000;
  }
};
```

---

## 8. Security Considerations

### AI Code Security Statistics (2025)

| Metric | Value | Source |
|--------|-------|--------|
| AI-generated code with security flaws | **45%** | Veracode |
| Java AI code with vulnerabilities | **70%** | Veracode |
| Python AI code with vulnerabilities | 30-40% | Veracode |
| Security scan failures for AI code | 4x higher than human | Industry Research |

> **Critical Finding:** Nearly half of all AI-generated code contains security vulnerabilities. Java has the highest failure rate at 70%, requiring extra scrutiny for enterprise Java projects.

### Prompt Injection Risks

**The Scale of the Problem:**
- **35%** of all real-world AI security incidents in 2025 caused by simple prompts
- Some incidents led to **$100K+** losses without writing a single line of code
- OpenAI admits prompt injection "is unlikely to ever be fully 'solved'"

**Real-World Incidents (2025):**
| Incident | Impact |
|----------|--------|
| Fortune 500 Financial Services | Customer service AI leaked sensitive data for weeks; millions in fines |
| [Salesforce Agentforce](https://noma.security/blog/forcedleak-agent-risks-exposed-in-salesforce-agentforce) | CVSS 9.4 vulnerability; CRM data exfiltration via prompt injection |
| Docker Hub AI Assistant | Data exfiltration via poisoned repository metadata |
| Amazon Q VS Code | Wiped local files, disrupted AWS infrastructure |
| [AI Browser Vulnerabilities](https://brave.com/blog/unseeable-prompt-injections/) | Invisible prompt injections in screenshots (systemic issue) |

### Attack Vectors

| Vector | Description | Mitigation |
|--------|-------------|------------|
| **File-based injection** | Malicious instructions in processed files | Input validation, sandboxing |
| **MCP exploitation** | Compromised MCP server | Server verification, access control |
| **Context poisoning** | Injecting instructions via clipboard | Context isolation |
| **Agent hijacking** | Redirecting autonomous agent behavior | Behavioral monitoring |

### Defense Layers

```
┌─────────────────────────────────────────────────────────────────┐
│                     SECURITY LAYERS                              │
│                                                                  │
│  1. INPUT VALIDATION                                             │
│     └── Scan processed documents for embedded instructions       │
│                                                                  │
│  2. AUTHORIZATION                                                │
│     └── Least-privilege access for AI agents                    │
│     └── Role-based access control                                │
│                                                                  │
│  3. BEHAVIORAL MONITORING                                        │
│     └── Baseline normal agent behavior                          │
│     └── Alert on anomalies                                       │
│                                                                  │
│  4. OUTPUT FILTERING                                             │
│     └── Block sensitive data exfiltration                       │
│     └── Prevent destructive commands                            │
│                                                                  │
│  5. AUDIT LOGGING                                                │
│     └── Log all AI tool actions                                 │
│     └── Enable forensic analysis                                │
└─────────────────────────────────────────────────────────────────┘
```

### Security Checklist

| Check | Implementation |
|-------|----------------|
| No secrets in prompts | Environment variables, secret managers |
| Limited file access | .codeiumignore, permission restrictions |
| No credential sharing | Never paste API keys to AI |
| MCP server verification | Review server source, limit permissions |
| Action logging | Track all AI-initiated actions |
| Anomaly detection | Monitor for unusual patterns |

### Secure Configuration

```json
// Example security configuration
{
  "security": {
    "allowedDirectories": [
      "src/",
      "tests/"
    ],
    "blockedPatterns": [
      "*.pem",
      "*.key",
      ".env*",
      "credentials/*"
    ],
    "commandWhitelist": [
      "npm test",
      "npm run lint",
      "git status"
    ],
    "requireApproval": [
      "npm publish",
      "git push",
      "rm -rf"
    ]
  }
}
```

---

## 9. Common Pitfalls

### Pitfall 1: Over-Reliance Without Verification

**The Problem:**
- **45%** of AI-generated code has security vulnerabilities (Veracode 2025)
- 62% of AI code has design flaws or vulnerabilities
- 70% more defects in AI PRs vs human PRs
- 76% of developers have low confidence in AI output

**The Solution:**
- Always review before commit
- Run comprehensive tests
- Use multi-stage validation pipeline

### Pitfall 2: Code Quality Degradation

**Pattern:** Individual velocity up, system quality down

**Why:** AI optimizes for "code that works" not "code that's maintainable"

**Signs:**
- Increasing tech debt
- More production incidents
- Harder to understand codebase

**Solution:**
- Invest in quality gates proportional to AI usage
- Static analysis on every PR
- Regular architectural review

### Pitfall 3: Architectural Drift

**Research Finding:** AI defaults to:
- Monolithic designs (vs microservices)
- Reimplementing vs using libraries
- "Vanilla style" without abstraction

**Solution:**
- Explicit architectural guidance in context
- Require library usage in prompts
- Regular architecture reviews

### Pitfall 4: Insufficient Context

**Signs:**
- AI generates code inconsistent with project style
- Frequent "that's not how we do it here"
- Low reuse of generated code

**Solution:**
- Invest in CLAUDE.md / .mdc files
- Create comprehensive instructions.md
- Regularly update context with learnings

### Pitfall 5: Shadow AI Usage

**Risk:** Developers using unapproved consumer AI tools

**Consequences:**
- Code exposure to third parties
- No enterprise security controls
- Compliance violations

**Solution:**
- Clear approved tool list
- Block consumer AI at network level
- Provide good alternatives

### Anti-Pattern Checklist

| Anti-Pattern | Detection | Fix |
|--------------|-----------|-----|
| Accept without review | No review comments on AI PRs | Require reviews |
| No tests for AI code | Low coverage on new files | Coverage gates |
| Ignoring AI mistakes | Same bugs recurring | Document limitations |
| Monolithic generation | Large, complex PRs | Break into smaller tasks |
| Copy-paste prompting | No context files | Invest in configuration |

---

## 11. Spec-Driven Development (SDD)

> **"Spec-driven development may not have the visibility of a term like vibe coding, but it's nevertheless one of the most important practices to emerge in 2025."** — [Thoughtworks](https://www.thoughtworks.com/en-ca/insights/blog/agile-engineering-practices/spec-driven-development-unpacking-2025-new-engineering-practices)

### What is Spec-Driven Development?

**Definition:** A development paradigm using well-crafted software specifications as prompts for AI coding agents to generate executable code.

**Why SDD Emerged:**
- Early AI tools (GitHub Copilot) generated only snippets
- "Vibe coding" produced unmaintainable, defective code
- Research showed specs + chain-of-thought = higher quality code
- 25% of Y Combinator Winter 2025 companies have 95% AI-generated codebases

```
┌─────────────────────────────────────────────────────────────────┐
│            VIBE CODING vs SPEC-DRIVEN DEVELOPMENT                │
│                                                                  │
│  VIBE CODING                    SPEC-DRIVEN                      │
│  ────────────                   ──────────                       │
│  "Just make it work"      →     Clear specifications             │
│  Trial and error          →     Requirements first               │
│  Tech debt accumulates    →     Maintainable architecture        │
│  AI guesses intent        →     AI follows explicit contract     │
│  Inconsistent quality     →     Predictable outcomes             │
└─────────────────────────────────────────────────────────────────┘
```

### SDD Three Maturity Levels

| Level | Description | Spec Lifecycle | Best For |
|-------|-------------|----------------|----------|
| **Spec-First** | Write spec before coding | Discarded after implementation | Small features |
| **Spec-Anchored** | Retain and evolve specs | Maintained during maintenance | Features needing updates |
| **Spec-as-Source** | Specs are primary artifact | Code regenerated from spec | Maximum consistency |

### SDD Tools Comparison (December 2025)

| Tool | Creator | Approach | Complexity | Best For |
|------|---------|----------|------------|----------|
| **[Kiro](https://kiro.dev)** | AWS/Amazon | Lightweight, 3-step | Low | Individual developers |
| **[Spec-Kit](https://github.com/github/spec-kit)** | GitHub | Constitution-based | Medium | Teams with standards |
| **[Tessl](https://tessl.io)** | Tessl | Spec-as-source | High | Full regeneration |

### GitHub Spec-Kit Workflow

**Installation:**
```bash
uvx --from git+https://github.com/github/spec-kit.git specify init <PROJECT_NAME>
```

**The 4 Phases:**

```
┌─────────────────────────────────────────────────────────────────┐
│                    SPEC-KIT WORKFLOW                             │
│                                                                  │
│  Phase 1: /specify                                               │
│  ├── Describe WHAT and WHY (not technical details)              │
│  ├── AI generates user journeys and success metrics             │
│  └── Output: Detailed specification document                     │
│                                                                  │
│  Phase 2: /plan                                                  │
│  ├── Define tech stack, architecture, constraints               │
│  ├── AI creates comprehensive technical plan                     │
│  └── Output: Implementation plan with architecture               │
│                                                                  │
│  Phase 3: /tasks                                                 │
│  ├── AI breaks spec+plan into concrete work units               │
│  ├── Each task: specific, reviewable, focused                   │
│  └── Output: Actionable task list                               │
│                                                                  │
│  Phase 4: /implement                                             │
│  ├── AI tackles tasks with spec/plan as guidance                │
│  ├── Focused changes, not thousand-line dumps                   │
│  └── Output: Working code                                        │
└─────────────────────────────────────────────────────────────────┘
```

**Spec-Kit Constitution (Team Standards):**
```markdown
<!-- .spec/constitution.md -->
# Engineering Standards

## Architecture
- Prefer composition over inheritance
- All external calls wrapped in service classes
- No business logic in controllers

## Security
- Input validation on all API endpoints
- SQL injection prevention via parameterized queries
- No secrets in code

## Testing
- Minimum 80% coverage for new code
- Integration tests for all API endpoints
- No mocking of core business logic

## Code Style
- Functions under 50 lines
- Classes under 300 lines
- Explicit typing on all public APIs
```

### Kiro Workflow (Lightweight SDD)

**Three Documents:**

```
.kiro/
├── requirements.md    # User stories: "As a... I want... So that..."
├── design.md          # Architecture, data flow, error handling
└── tasks.md           # TODOs linked to requirement numbers
```

**Requirements Format (Given/When/Then):**
```markdown
## Feature: User Authentication

### Story 1: Login
As a registered user
I want to log in with my credentials
So that I can access my account

#### Acceptance Criteria
**GIVEN** a registered user with valid credentials
**WHEN** they submit the login form
**THEN** they receive a JWT token AND are redirected to dashboard

**GIVEN** a user with invalid credentials
**WHEN** they submit the login form
**THEN** they see an error message AND no token is issued
```

### SDD with Claude Code

**Setup for Claude Code:**
```markdown
<!-- CLAUDE.md addition -->
## Development Methodology: Spec-Driven Development

### Workflow
1. Always request/create specs BEFORE implementing
2. Use Requirements → Design → Tasks → Implementation flow
3. Validate each phase before proceeding

### Spec Location
- Requirements: docs/specs/[feature]-requirements.md
- Design: docs/specs/[feature]-design.md
- Tasks: docs/specs/[feature]-tasks.md

### Spec Template
When creating a new feature:
1. Generate requirements with Given/When/Then scenarios
2. Design architecture including error handling and tests
3. Break into focused tasks (1-2 hours each)
4. Implement one task at a time with verification
```

**Custom Command for SDD:**
```markdown
<!-- .claude/commands/spec.md -->
# Create Feature Specification

Generate a complete specification for: $ARGUMENTS

## Requirements Document
- User stories with acceptance criteria
- Given/When/Then scenarios for each story
- Edge cases and error conditions

## Design Document
- Component architecture
- Data flow diagram (as ASCII)
- API contracts
- Error handling strategy
- Test approach

## Task Breakdown
- Numbered tasks linked to requirements
- Estimated complexity (S/M/L)
- Dependencies between tasks

Save to: docs/specs/$ARGUMENTS/
```

### SDD Best Practices

**From [Martin Fowler's Research](https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html):**

| Practice | Guidance |
|----------|----------|
| **Scalability** | Match workflow complexity to task size |
| **Review efficiency** | Prefer readable specs over verbose docs |
| **Iteration** | Balance upfront spec with small increments |
| **Functional-technical separation** | Clear boundaries in specs |
| **Non-determinism** | Iterate specs when AI output varies |

**What Makes Good Specs:**
- Use domain language (not tech jargon)
- Given/When/Then for clear scenarios
- Complete but concise
- Machine-readable structure

**When to Use SDD:**

| Scenario | Use SDD? | Approach |
|----------|----------|----------|
| New feature | Yes | Full spec cycle |
| Bug fix | Optional | Minimal spec |
| Refactoring | Yes | Design-focused spec |
| Quick prototype | No | Vibe code, then spec |
| Legacy modernization | Yes | Capture business logic |

**Source:** [GitHub Spec-Kit](https://github.com/github/spec-kit), [Thoughtworks SDD Guide](https://www.thoughtworks.com/en-ca/insights/blog/agile-engineering-practices/spec-driven-development-unpacking-2025-new-engineering-practices), [Martin Fowler: Exploring GenAI](https://martinfowler.com/articles/exploring-gen-ai.html)

---

## 12. Context Engineering

> **"Context engineering is the new backbone of scalable AI systems."** — [Anthropic Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

### What is Context Engineering?

**Definition:** The set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference.

**The Evolution:**
```
Prompt Engineering (2022-2024)
├── Focus: Writing better instructions
└── Scope: Single prompts

Context Engineering (2025+)
├── Focus: Managing entire context state
└── Scope: All information reaching the model
```

**The Problem Context Engineering Solves:**
- Agents on long tasks accumulate history, tool outputs, documents
- Context windows have finite "attention budgets"
- Performance degrades as context expands ("context rot")
- Transformers struggle: n² pairwise token relationships

### Context Engineering Principles

```
┌─────────────────────────────────────────────────────────────────┐
│           CONTEXT ENGINEERING CORE PRINCIPLES                    │
│                                                                  │
│  1. MINIMAL CONTEXT                                              │
│     └── Smallest collection of high-signal tokens                │
│                                                                  │
│  2. RIGHT ALTITUDE                                               │
│     └── Specific enough to guide, flexible enough to adapt       │
│                                                                  │
│  3. STRUCTURED SECTIONS                                          │
│     └── XML tags or Markdown headers for organization            │
│                                                                  │
│  4. PROGRESSIVE DISCLOSURE                                       │
│     └── Load information incrementally, not all upfront          │
│                                                                  │
│  5. TOKEN EFFICIENCY                                             │
│     └── Every token must earn its place                          │
└─────────────────────────────────────────────────────────────────┘
```

### Just-in-Time Context Retrieval

**Instead of:** Pre-loading all potentially relevant data

**Do:** Maintain lightweight identifiers and load dynamically

```python
# Anti-pattern: Loading everything upfront
context = load_entire_codebase()  # 500K tokens
response = llm.invoke(context + query)

# Pattern: Just-in-time retrieval
file_index = build_file_index()  # Lightweight paths/summaries
relevant_files = llm.identify_relevant(file_index, query)
context = load_specific_files(relevant_files)  # 10K tokens
response = llm.invoke(context + query)
```

**Benefits:**
- Reduces token waste
- Enables progressive discovery
- Uses metadata signals (folder hierarchies, naming conventions)

**Trade-offs:**
- Runtime exploration is slower than pre-computed retrieval
- Requires thoughtful tool design

**Hybrid Approach (Recommended):**
```
CLAUDE.md → Static context (always loaded)
    +
glob/grep → Just-in-time file access (loaded when needed)
```

### Structured Context Patterns for Long Tasks

**Pattern 1: Compaction**

Summarize conversation history when approaching limits:

```markdown
<!-- Before compaction: 50K tokens -->
[Full conversation history with all tool outputs]

<!-- After compaction: 5K tokens -->
## Session Summary
- Implemented user authentication (JWT-based)
- Created login/logout endpoints
- Added rate limiting middleware
- Remaining: Password reset feature

## Key Decisions
- Chose bcrypt for password hashing (security)
- 24-hour token expiry (UX/security balance)

## Current State
- Working: /login, /logout, /profile
- Pending: /reset-password, /change-password
```

**Pattern 2: Structured Note-Taking**

Maintain persistent notes outside context window:

```markdown
<!-- .claude/session-notes.md -->
# Project Notes

## Architecture Decisions
- 2025-12-26: Chose PostgreSQL over MongoDB (ACID compliance needed)
- 2025-12-26: Redis for session cache (speed priority)

## Implementation Progress
- [x] Database schema
- [x] Auth endpoints
- [ ] Email service integration

## Blockers
- Email provider API key not configured
```

**Pattern 3: Sub-Agent Architecture**

Delegate focused tasks to specialized sub-agents:

```
┌─────────────────────────────────────────────────────────────────┐
│                    SUB-AGENT ARCHITECTURE                        │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │                   ORCHESTRATOR AGENT                      │   │
│  │  Clean context window, coordinates sub-agents             │   │
│  └──────────────────────────────────────────────────────────┘   │
│           │              │              │                        │
│           ▼              ▼              ▼                        │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐                │
│  │  RESEARCH  │  │  IMPLEMENT │  │   TEST     │                │
│  │  SUB-AGENT │  │  SUB-AGENT │  │  SUB-AGENT │                │
│  │            │  │            │  │            │                │
│  │  Explores  │  │  Writes    │  │  Validates │                │
│  │  codebase  │  │  code      │  │  changes   │                │
│  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘                │
│        │               │               │                        │
│        └───────────────┼───────────────┘                        │
│                        ▼                                        │
│            Returns 1-2K token summary                           │
│            (not full exploration history)                       │
└─────────────────────────────────────────────────────────────────┘
```

**Sub-agent returns:** Condensed summary (1,000-2,000 tokens), not full exploration

### Context Strategy Selection

| Task Characteristic | Best Strategy |
|---------------------|---------------|
| Extensive back-and-forth | Compaction |
| Iterative development with milestones | Note-taking |
| Complex research + parallel exploration | Multi-agent |
| Quick lookup, known location | Just-in-time |
| Always-needed project info | Static (CLAUDE.md) |

### Tool Design for Context Efficiency

**Anti-Pattern: Bloated Tool Results**
```python
# Bad: Returns entire file
def read_file(path):
    return open(path).read()  # Could be 10K+ tokens
```

**Pattern: Efficient Tool Results**
```python
# Good: Returns targeted content
def read_file(path, start_line=None, end_line=None, search=None):
    content = open(path).readlines()
    if search:
        return [l for l in content if search in l]
    if start_line and end_line:
        return content[start_line:end_line]
    return content[:100]  # Default: first 100 lines
```

### Observation Masking

Clear old tool results to save tokens:

```
Before: "File auth.ts was read. Contents: [2000 tokens of code]..."
After:  "File auth.ts was read at step 47"
```

**Implementation:**
```python
def mask_old_observations(conversation, keep_last_n=3):
    """Replace old tool outputs with references."""
    for i, msg in enumerate(conversation[:-keep_last_n]):
        if msg.role == "tool_result":
            msg.content = f"[Tool output from step {i}]"
    return conversation
```

### Context Engineering for Teams

**Shared Context Files:**
- CLAUDE.md / .mdc rules → Team standards
- .spec/constitution.md → Architectural principles
- docs/decisions/ → ADRs (Architecture Decision Records)

**Context Budget Allocation:**
```
┌─────────────────────────────────────────────────────────────────┐
│            RECOMMENDED TOKEN BUDGET (200K window)                │
│                                                                  │
│  System Prompt + Rules     │██████████████░░░░░│  10-15%        │
│  Project Context           │█████████░░░░░░░░░░│  5-10%         │
│  Current Task Spec         │██████████░░░░░░░░░│  5-10%         │
│  Relevant Code             │███████████████████│  40-50%        │
│  Conversation History      │████████░░░░░░░░░░░│  10-15%        │
│  Working Memory (buffer)   │██████░░░░░░░░░░░░░│  10-20%        │
└─────────────────────────────────────────────────────────────────┘
```

**Source:** [Anthropic: Effective Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents), [Manus: Context Engineering Lessons](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)

---

## 13. Quick Reference

### Tool Selection

| Need | Tool |
|------|------|
| Full IDE replacement | Cursor or Windsurf |
| Terminal-first workflow | Claude Code |
| Existing VS Code user | GitHub Copilot |
| Autonomous task execution | Devin |
| Quick prototyping | Any (Cursor fastest) |

### Keyboard Shortcuts Cheat Sheet

**Cursor:**
| Action | Shortcut |
|--------|----------|
| Chat | Cmd+L |
| Composer | Cmd+I |
| Inline Edit | Cmd+K |
| Terminal | Cmd+` |

**Claude Code:**
| Action | Shortcut |
|--------|----------|
| Add to memory | # [text] |
| Custom command | /[command] |
| Clear context | /clear |

### Development Workflows

**RIPER Framework:**
```
Research → Innovate → Plan → Execute → Review
Key: ZERO UNAUTHORIZED CHANGES
```

**Spec-Driven Development:**
```
Specify → Plan → Tasks → Implement
Tools: GitHub Spec-Kit, Kiro, Tessl
```

**Context Engineering:**
```
Minimal Context + Just-in-Time Retrieval + Compaction
Budget: 40-50% for relevant code, 10-15% for context
```

### Configuration Quick Start

**Cursor:**
```bash
mkdir -p .cursor/rules
touch .cursor/rules/project.mdc
```

**Claude Code:**
```bash
touch CLAUDE.md
mkdir -p .claude/commands
```

**Windsurf:**
```bash
touch .codeiumignore
```

**Spec-Kit:**
```bash
uvx --from git+https://github.com/github/spec-kit.git specify init <PROJECT>
```

### Testing Strategy

```
1. Generate comprehensive tests FIRST
2. Implement to pass tests
3. Review generated tests for correctness
4. Run with coverage
5. Add missed edge cases
```

### Security Essentials

| Rule | Implementation |
|------|----------------|
| Never share secrets | Use env vars |
| Limit file access | Use ignore files |
| Review all output | Human check required |
| Log everything | Audit trail |
| Use approved tools | No shadow AI |

### Cost Optimization

| Technique | Savings |
|-----------|---------|
| Static content first | 50-90% (caching) |
| Concise prompts | 20-30% |
| Smaller models when possible | 40-60% |
| Batch requests | 50% |

### Team Setup Checklist

- [ ] Define approved tools list
- [ ] Create governance policy
- [ ] Set up shared configuration files
- [ ] Train team on best practices
- [ ] Establish code review process
- [ ] Configure security controls
- [ ] Set up cost monitoring
- [ ] Create shared prompt library

---

## Related Documents

- [agent-prompting-guide.md](agent-prompting-guide.md) - Detailed prompting techniques
- [patterns-and-antipatterns.md](patterns-and-antipatterns.md) - Common failures
- [security-essentials.md](security-essentials.md) - Security implementation
- [api-optimization-guide.md](api-optimization-guide.md) - Cost optimization

---

**Document Version:** 1.0
**Last Updated:** December 2025
**Sources:** JetBrains, CodeRabbit, Qodo, Cloud Security Alliance, OX Security, Cognition, Fortune, 30+ industry reports
